---
title: "List 1"
author: "Dawid Dieu"
date: '2022-03-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Problem 1
### Generate data
```{r}
set.seed(2022)
N = 1000
m = 950
generate_data <- function() {
  X = matrix(
    data=rnorm(n=N * m, mean=0, sd = 1 / sqrt(N)), 
    nrow=N, 
    ncol=m,
  )
  
  Beta = rep(x=0, times=m)
  Beta[0: 5] = 3
  
  eps = rnorm(n=N, mean=0, sd=1)
  
  Y = X %*% Beta + eps
  
  return (list(X, Y))
}

```

### Perform the following analysis using reduced models
```{r}
set.seed(2022)
print_fdr_info <- function(tp, fp, prefix) {
  cat(prefix, 'True Positives: ', tp , '\n')
  cat(prefix, 'False Positives: ', fp , '\n')
  fdr = fp / (fp + tp + 10^-9) * 100
  cat(prefix, 'False discovery rate (FDR): ', fdr, '%\n')
}


run_test <- function(show_info=TRUE) {
  n_features = c(10, 100, 500, 950)
  new_data = generate_data()
  X = new_data[[1]]
  Y = new_data[[2]]
  
  for (n in n_features) {
    if(show_info) cat('\n📊Test for', n, 'columns\n')
    if(show_info) cat('Shape of X:', dim(X[, 1:n]), '\n')
    
    model = lm(formula=Y ~ X[, 1:n])
    Beta_hat = model $ coefficients  # least squares estimator of Beta from lm
    p_values = summary(model) $ coefficients[, 4]
    
    # add vector of 1s which are coefficients of the intercept
    new_X = cbind(rep(x=1, times=N), X[, 1:n])
    
    # least squares estimator of Beta from closed form solution
    myBeta = solve(a = t(new_X) %*% new_X) %*% t(new_X) %*% Y
    
    diff = sum((Beta_hat - myBeta) ^ 2)
    if(show_info) cat('Beta (my manual calculation) vs Beta from lm SSE difference: ', diff, '\n')
    
    #  a) tests for significance of individual regression coefficients
    alpha = 0.1
    discoveries = p_values < alpha
    if(show_info) cat('🔸a) Number of significant Beta_hat: ', sum(discoveries), '\n')
  
    # b) average standard deviation of the estimators of individual regression coefficients
    Y_hat = new_X %*% Beta_hat
    SSE = t((Y - Y_hat)) %*% (Y - Y_hat)
    dfe = 1000 -  n # degrees of freedom
    MSE = (SSE / dfe)[1]
    cov_matrix = MSE * solve(t(new_X) %*% new_X)
    variances = diag(cov_matrix)
    sigmas = sqrt(variances)
    if(show_info) cat('🔹b) Average standard deviation (my manual calculation): ', mean(sigmas), '\n')
    
    sigmas_from_summary = summary(model) $ coefficients[, 2]
    if(show_info) cat('🔹b) Average standard deviation (from model summary): ', mean(sigmas), '\n')
    
    # average length of the 90% confidence intervals
    t = qt(p = 1 - alpha / 2, df = 1000 - n)
    interval = t * sigmas * 2
    if(show_info) cat('🔹b) Average length of the 90% confidence intervals: ', mean(interval), '\n')
    
    # True and False discoveries without adjusting for multiple testing
    true_positives = sum(discoveries[2:6])
    # we add discoveries[1] because we don't use intercept in our model
    false_positives = as.integer(discoveries[1])  + sum(discoveries[7:length(discoveries)])
    if(show_info) cat('🔸c) i) Without adjusting for multiple testing\n')
    if(show_info) print_fdr_info(tp=true_positives, fp=false_positives, prefix='🔸c) i)')
    
    # sort p values
    sorted_indices = order(p_values)
    
    # Bonferroni correction; reject i-th coefficient if p < alpha / n
    if(show_info) cat('🔸c) ii) Bonferroni correction\n')
    bonferroni_rejected_indices = sorted_indices[p_values[sorted_indices] < (alpha / n)]
    true_positives = sum(bonferroni_rejected_indices >= 2 & bonferroni_rejected_indices <= 6)
    false_positives = sum(bonferroni_rejected_indices == 1 | bonferroni_rejected_indices >= 7)
    if(show_info) print_fdr_info(tp=true_positives, fp=false_positives, prefix='🔸c) ii) [Bonferroni]')
  
    # Benjamini-Hochberg correction; reject i-th coefficient if p_i < i * alpha / n
    if(show_info) cat('🔸c) iii) Benjamini-Hochberg correction\n')
    bh_rejected_indices = sorted_indices[
      p_values[sorted_indices] < (alpha / n) * seq(from=1, to=n + 1, by=1)
    ]
    true_positives = sum(bh_rejected_indices >= 2 & bh_rejected_indices <= 6)
    false_positives = sum(bh_rejected_indices == 1 | bh_rejected_indices >= 7)
    if(show_info) print_fdr_info(
      tp=true_positives, 
      fp=false_positives, 
      prefix='🔸c) iii) [Benjamini-Hochberg]'
    )
  }
}

run_test(show_info=TRUE)
```
## Problem 1

### a)
#### Number of significant coefficients for each model with $\alpha=0.1$

| number of columns | number of significant coefficients |
|:-----------------:|:-------------------------------:|
|         10        |                4                |
|        100        |                11               |
|        500        |                45               |
|        950        |               105               |

### b)
#### Average standard deviation of the estimators of individual regression coefficients and the average length of the respective 90% confidence intervals

| number of columns | average standard deviation | average length of 90% confidence intervals |
|:-----------------:|:--------------------------:|:------------------------------------------:|
|         10        |          0.9087013         |                  2.992161                  |
|        100        |          1.038993          |                  3.421506                  |
|        500        |           1.39654          |                  4.602734                  |
|        950        |          4.366425          |                  14.63543                  |

### c)
#### False Discoveries Correction
##### i) Without adjusting for multiple testing

| number of columns | True Positives | False Positives |   FDR  |
|:-----------------:|:--------------:|:---------------:|:------:|
|         10        |        4       |        0        |   0%   |
|        100        |        5       |        6        | 54.54% |
|        500        |        2       |        43       | 95.55% |
|        950        |        1       |       104       | 99.04% |


##### ii) using Bonferroni correction

| number of columns | True Positives | False Positives | FDR |
|:-----------------:|:--------------:|:---------------:|:---:|
|         10        |        2       |        0        |  0% |
|        100        |        1       |        0        |  0% |
|        500        |        0       |        0        |  0% |
|        950        |        0       |        0        |  0% |


##### ii) using Benjamini-Hochberg correction

| number of columns | True Positives | False Positives | FDR |
|:-----------------:|:--------------:|:---------------:|:---:|
|         10        |        3       |        0        |  0% |
|        100        |        1       |        0        |  0% |
|        500        |        0       |        0        |  0% |
|        950        |        0       |        0        |  0% |



## Problem 2
### Repeat the above experiments 500 times

```{r}
run_multiple_tests <- function() {
  n_features = c(10, 100, 500, 950)
  
  for (n in n_features) {
    N_TESTS = 500
    cat('\n📊Performing ', N_TESTS, 'tests for', n, 'columns\n')
       
    alpha = 0.1
    
    experimental_variances = c()
    experimental_length_of_interval = c()
    
    true_positives_no_adjusting = 0
    false_positives_no_adjusting = 0
    fdr_no_adjusting = 0
    fwer_no_adjusting = 0
    
    true_positives_bonferroni = 0
    false_positives_bonferroni = 0
    fdr_bonferroni = 0
    fwer_bonferroni = 0
    
    true_positives_bh = 0
    false_positives_bh = 0
    fdr_bh = 0
    fwer_bh = 0
    
    start_time <- Sys.time()
    
    for (t in 1:N_TESTS) {
      new_data = generate_data()
      X = new_data[[1]]
      Y = new_data[[2]]
    
      new_X = cbind(rep(x=1, times=N), X[, 1:n])
      model = lm(formula=Y ~ X[, 1:n])
 
      Beta_hat = model $ coefficients  # least squares estimator of Beta from lm
      p_values = summary(model) $ coefficients[, 4]
      discoveries = p_values < alpha
    
      # a)  average variance of the estimators of individual regression coefficients
      Y_hat = new_X %*% Beta_hat
      SSE = t((Y - Y_hat)) %*% (Y - Y_hat)
      dfe = N -  n # degrees of freedom
      MSE = (SSE / dfe)[1]
      cov_matrix = MSE * solve(t(new_X) %*% new_X)
      variances = diag(cov_matrix)
      sigmas = sqrt(variances)
      experimental_variances = c(experimental_variances, mean(variances))
      
      sigmas_from_summary = summary(model) $ coefficients[, 2]
      
      # b) average length of the 90% confidence intervals
      t = qt(p = 1 - alpha / 2, df = N - n)
      interval = t * sigmas * 2
      experimental_length_of_interval = c(experimental_length_of_interval, mean(interval))
      
      # c) True and False discoveries without adjusting for multiple testing
      true_positives = sum(discoveries[2:6])
      # we add discoveries[1] because we don't use intercept in our model
      false_positives = as.integer(discoveries[1])  + sum(discoveries[7:length(discoveries)])
      true_positives_no_adjusting = true_positives_no_adjusting + true_positives
      false_positives_no_adjusting = false_positives_no_adjusting + false_positives
      fdr = false_positives / (false_positives + true_positives + 10^-9)
      fdr_no_adjusting = fdr_no_adjusting + fdr
      if (false_positives > 0) {
        fwer_no_adjusting = fwer_no_adjusting + 1
      }
      
      # sort p values
      sorted_indices = order(p_values)
      
      # Bonferroni correction; reject i-th coefficient if p < alpha / n
      bonferroni_rejected_indices = sorted_indices[p_values[sorted_indices] < (alpha / n)]
      true_positives = sum(bonferroni_rejected_indices >= 2 & bonferroni_rejected_indices <= 6)
      false_positives = sum(bonferroni_rejected_indices == 1 | bonferroni_rejected_indices >= 7)
      true_positives_bonferroni = true_positives_bonferroni + true_positives
      false_positives_bonferroni = false_positives_bonferroni + false_positives
      fdr = false_positives / (false_positives + true_positives + 10^-9)
      fdr_bonferroni = fdr_bonferroni + fdr
      if (false_positives > 0) {
        fwer_bonferroni = fwer_bonferroni + 1
      }
    
      # Benjamini-Hochberg correction; reject i-th coefficient if p_i < i * alpha / n
      bh_rejected_indices = sorted_indices[
        p_values[sorted_indices] < (alpha / n) * seq(from=1, to=n + 1, by=1)
      ]
      true_positives = sum(bh_rejected_indices >= 2 & bh_rejected_indices <= 6)
      false_positives = sum(bh_rejected_indices == 1 | bh_rejected_indices >= 7)
      true_positives_bh = true_positives_bh + true_positives
      false_positives_bh = false_positives_bh + false_positives
      fdr = false_positives / (false_positives + true_positives + 10^-9)
      fdr_bh = fdr_bh + fdr
      if (false_positives > 0) {
        fwer_bh = fwer_bh + 1
      }
    }
    theoretical_variance = (N / (N - n - 1))
    cat('🔹a) Theoretical average variance: ', theoretical_variance, '\n')
    cat('🔹a) Experimental average variance: ', mean(experimental_variances), '\n')
  
    t = qt(p = 1 - alpha / 2, df = N - n)
    theoretical_lenght = t * sqrt(theoretical_variance) * 2
    cat('🔸b) Theoretical average length of 90% CI: ', theoretical_lenght, '\n')
    cat('🔸b) Experimental average length of 90% CI: ', mean(experimental_length_of_interval), '\n')
    cat('=====\n')
    cat('🔹c) True positives without adjusting for multiple testing: ', true_positives_no_adjusting / N_TESTS, '\n')
    cat('🔹c) False positives without adjusting for multiple testing: ', false_positives_no_adjusting / N_TESTS, '\n')
    cat('🔹c) FDR without adjusting for multiple testing: ', fdr_no_adjusting / N_TESTS, '\n')
    cat('🔹c) FWER without adjusting for multiple testing: ', fwer_no_adjusting / N_TESTS, '\n')
    theoretical_fwer_no_adjusting = 1 - (1 - alpha) ^ (n - 5)
    cat('🔹c) Theoretical FWER without adjusting for multiple testing: ', theoretical_fwer_no_adjusting, '\n')
    cat('=====\n')
    cat('🔹c) True positives using Bonferroni correction (all): ', true_positives_bonferroni, '\n')
    cat('🔹c) True positives using Bonferroni correction (mean): ', true_positives_bonferroni / N_TESTS, '\n')
    cat('🔹c) False positives using Bonferroni correction (all): ', false_positives_bonferroni, '\n')
    cat('🔹c) False positives using Bonferroni correction (mean): ', false_positives_bonferroni / N_TESTS, '\n')
    cat('🔹c) FDR using Bonferroni correction: ', fdr_bonferroni / N_TESTS, '\n')
    cat('🔹c) FWER using Bonferroni correction: ', fwer_bonferroni / N_TESTS, '\n')
    theoretical_fwer_bonferroni = alpha * (n - 5) / n
    cat('🔹c) Theoretical FWER using Bonferroni correction: ', theoretical_fwer_bonferroni, '\n')
    cat('=====\n')
    cat('🔹c) True positives using Benjamini-Hochberg correction (all): ', true_positives_bh, '\n')
    cat('🔹c) True positives using Benjamini-Hochberg correction (mean): ', true_positives_bh / N_TESTS, '\n')
    cat('🔹c) False positives using Benjamini-Hochberg correction (all): ', false_positives_bh, '\n')
    cat('🔹c) False positives using Benjamini-Hochberg correction (mean): ', false_positives_bh / N_TESTS, '\n')
    cat('🔹c) FDR using Benjamini-Hochberg correction: ', fdr_bh / N_TESTS, '\n')
    cat('🔹c) FWER using Benjamini-Hochberg correction: ', fwer_bh / N_TESTS, '\n')
    
    cat('🏁 Finished in: ', Sys.time() - start_time, 'seconds 🏁\n\n')
  }
}

run_multiple_tests()
```

### a)
#### Compare average variance of the estimators of individual regression coefficients with the theoretical values (using the inverse Wishart distribution)

| number of columns | variance of the estimators | theoretical values |
|:-----------------:|:--------------------------:|:------------------:|
|         10        |            0.917           |        1.011       |
|        100        |            1.098           |        1.112       |
|        500        |           2.007            |       2.004        |
|        950        |              0             |          0         |


### b)
#### Average length of the 90% confidence interval

| number of columns | average length of 90% CI | theoretical values |
|:-----------------:|:------------------------:|:------------------:|
|         10        |           3.015          |        3.311       |
|        100        |           3.433          |        3.473       |
|        500        |           4.66           |        4.66        |
|        950        |             0            |          0         |

### c)
#### The average number of true and false discoveries and the estimators of the False Discovery Rate (FDR) and the Family Wise Error Rate (FWER)
#### For the procedures without the correction and with the Bonferroni I also provide theoretical FWER

##### i) without adjusting for multiple testing

| number of columns | True positives rate | False Positives rate |  FDR  |  FWER | Theoretical FWER |
|:-----------------:|:-------------------:|:--------------------:|:-----:|:-----:|:----------------:|
|         10        |        4.622        |      0.584           | 0.097 | 0.458 |       0.409      |
|        100        |        4.392        |         9.78         | 0.674 |   1   |       0.999      |
|        500        |        3.442        |        49.132        | 0.932 |   1   |         1        |
|        950        |        0.894        |        95.934        | 0.989 |   1   |         1        |


##### ii) using Bonferroni correction


| number of columns | True positives rate | False Positives rate |   FDR  |  FWER | Theoretical FWER |
|:-----------------:|:-------------------:|:--------------------:|:------:|:-----:|:----------------:|
|         10        |        3.332        |         0.068        |  0.164 | 0.066 |       0.05       |
|        100        |         1.72        |         0.116        |  0.046 | 0.114 |       0.995      |
|        500        |        0.272        |         0.088        |  0.069 | 0.084 |       0.099      |
|        950        |        0.006        |         0.096        | 0.0677 | 0.068 |      0.0994      |


##### iii) using Benjamini-Hochberg correction


| number of columns | True positives rate | False Positives rate |   FDR  |  FWER |
|:-----------------:|:-------------------:|:--------------------:|:------:|:-----:|
|         10        |        4.244        |         0.332        |  0.058 | 0.274 |
|        100        |        2.284        |         0.338        | 0.0853 | 0.256 |
|        500        |        0.326        |         0.156        | 0.0911 | 0.122 |
|        950        |        0.016        |         0.698        | 0.0787 |  0.08 |






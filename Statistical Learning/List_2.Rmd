---
title: "List_2"
author: "Febrin"
date: '2022-03-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Task 1
```{r}
set.seed(2022)

N = 1000
m = 950

X = matrix(
  data=rnorm(n=N * m, mean=0, sd = 1 / sqrt(N)), 
  nrow=N, 
  ncol=m,
)

Beta = rep(x=0, times=m)
Beta[0: 5] = 3

eps = rnorm(n=N, mean=0, sd=1)

Y = X %*% Beta + eps
```

#### Perform the following analysis using reduced models
```{r}
n_features = c(10, 100, 500, 950)

for (n in n_features) {
  cat('\nTest for', n, ' columns\n')
  cat('Shape of X:', dim(X[, 1:n]), '\n')
  
  model = lm(formula=Y ~ X[, 1:n])
  # print(names(summary(model)))
  Theta = model $ coefficients  # least squares estimator of Beta from lm
  
  new_X = cbind(rep(x=1, times=N), X[, 1:n])
  Y_hat = new_X %*% Theta
  
  RSS = sum((Y - Y_hat) ^ 2)
  cat('RSS = ', RSS, '\n')
  
  # true expected value of the prediction error
  cat('PE = sigma^2(N - n) + N =', (1 / N) * (N - n) + N, '\n')
  
  # Use the residual sum of squares to estimate PE
  MSE = RSS / (N - n)  # = estimated sigma ^ 2
  PE = MSE * (N - n) + N
  cat('estimated PE = ', PE, '\n')
  
  # Estimate P E using leave-one-out crossvalidation 
  M = new_X %*% solve(t(new_X) %*% new_X) %*% t(new_X)
  CV = sum(((Y - Y_hat) / (1 - diag(M)))^2)
  # cat('estimated CV = ', CV, '\n')
  cat('estimated PE using CV = ', CV + N , '\n')
  
  A = t(new_X) %*% new_X
  I = A %*% solve(A)
  print(sum(diag(I)))
  print(sum(I))
}

```
 


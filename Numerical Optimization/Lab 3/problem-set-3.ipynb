{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Optimization\" data-toc-modified-id=\"Numerical-Optimization-1\">Numerical Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-set-3-(14-points)\" data-toc-modified-id=\"Problem-set-3-(14-points)-1.1\">Problem set 3 (14 points)</a></span></li><li><span><a href=\"#Bonus-(4-points)\" data-toc-modified-id=\"Bonus-(4-points)-1.2\">Bonus (4 points)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafa≈Ç Nowak\n",
    "# Numerical Optimization\n",
    "## Problem set 3 (14 points)\n",
    "\n",
    "**Submission deadline**: Thursday, 02.02.2022\n",
    "\n",
    "* All submissions should contain single file.<br/>This can be single Jupyter notebook file (with extension `ipynb`) or ZIP archive in case the are some additional files needed.\n",
    "* It is recommended to write the reports using LaTeX. \n",
    "* One can report the answers, comments and results in PDF or notebook file.\n",
    "* All the source code should be written in Python or Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.1\n",
    "(total 8 pts)\n",
    "\n",
    "(2 pts) Complete the implementation of Newton's method (see [Boyd, *Convex Optimization*, $\\S 9.5.2$])\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see the Newton's methond definition.</summary>\n",
    "  <img src=\"https://i.ibb.co/RvqY16d/Boyd-Newton-method.png\" alt=\"Boyd-Newton-method\">\n",
    "</details>\n",
    "\n",
    "#### Remark: Implement bisection method first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation and compare the results for the following functions\n",
    "* (1 pts) function $ f(x) = x^4 + 16x^2 + 18(x-4) e^x\\qquad (x\\in\\mathbb R). $\n",
    "\n",
    "\n",
    "* (1 pt) function `boyd_example_func` written in Python below\n",
    "\n",
    "\n",
    "* (2 pts) the following Python function `quadratic` (with different types of matrix $H$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark. In `newton` function you should use both `exact_line_search` (1 pt) and `backtracking` (1 pt).\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see the exact_line_search and backtracking definitions.</summary>\n",
    "  <img width=\"80%\" src=\"https://i.ibb.co/1fQ0Nfs/Boyd-line-search.png\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boyd_example_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boyd_example_func(x, order=0):\n",
    "    a = np.matrix('1  3')\n",
    "    b = np.matrix('1  -3')\n",
    "    c = np.matrix('-1  0')\n",
    "    x = np.asmatrix(x)\n",
    "\n",
    "    value = exp(a*x-0.1)+exp(b*x-0.1)+exp(c*x-0.1)\n",
    "    if order==0:\n",
    "        return value\n",
    "    elif order==1:\n",
    "        gradient = a.T*exp(a*x-0.1)+b.T*exp(b*x-0.1)+c.T*exp(c*x-0.1)\n",
    "        return (value, gradient)\n",
    "    elif order==2:\n",
    "        gradient = a.T*exp(a*x-0.1)+b.T*exp(b*x-0.1)+c.T*exp(c*x-0.1)\n",
    "        hessian = a.T*a*exp(a*x-0.1)+b.T*b*exp(b*x-0.1)+c.T*c*exp(c*x-0.1)\n",
    "        return (value, gradient, hessian)\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic( H, b, x, order=0 ):\n",
    "    \"\"\" \n",
    "    Quadratic Objective\n",
    "    H:          the Hessian matrix\n",
    "    b:          the vector of linear coefficients\n",
    "    x:          the current iterate\n",
    "    order:      the order of the oracle. For example, order=1 returns the value of the function and its gradient while order=2 will also return the hessian\n",
    "    \"\"\"\n",
    "    H = np.asmatrix(H)\n",
    "    b = np.asmatrix(b)\n",
    "    x = np.asmatrix(x)\n",
    "    \n",
    "    value = 0.5 * x.T * H * x + b.T * x\n",
    "\n",
    "    if order == 0:\n",
    "        return value\n",
    "    elif order == 1:\n",
    "        gradient = H * x + b\n",
    "        return (value, gradient)\n",
    "    elif order == 2:\n",
    "        gradient = H * x + b\n",
    "        hessian = H\n",
    "        return (value, gradient, hessian)\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisec(\n",
    "\n",
    "\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(\n",
    "    func,\n",
    "    initial_x, \n",
    "    eps=1e-5, \n",
    "    maximum_iterations=65536, \n",
    "    linesearch=bisection, \n",
    "    *linesearch_args  \n",
    "):\n",
    "    \"\"\" \n",
    "    Newton's Method\n",
    "    func:               the function to optimize It is called as \"value, gradient, hessian = func( x, 2 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "    \n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    \n",
    "    # Newton's method updates\n",
    "    while True:\n",
    "        \n",
    "        value, gradient, hessian = func( x , order=2 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.matrix( gradient )\n",
    "        hessian = np.matrix( hessian ) \n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        ### TODO: Compute the Newton update direction\n",
    "        direction = np.array((-hessian_inv @ gradient.T).T)[0]\n",
    "\n",
    "        ### TODO: Compute the Newton decrement\n",
    "        newton_decrement = gradient @ hessian_inv @ gradient.T\n",
    "\n",
    "\n",
    "        if newton_decrement / 2 <= eps:   ### TODO: TERMINATION CRITERION\n",
    "            break\n",
    "        \n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "\n",
    "        ### TODO: update x\n",
    "        x = x + _____________\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.2\n",
    "(total 6 pts)\n",
    "\n",
    "(2 pts) Complete the implementation of Conjugate gradients method (see [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$])\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see the Congugate gradient method.</summary>\n",
    "  <img src=\"https://i.ibb.co/Hxn9PmM/Nocedal-Wright-CG-FR.png\">\n",
    "</details>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "###############################################################################\n",
    "def cg( func, initial_x, eps=1e-5, maximum_iterations=65536, linesearch=bisection, *linesearch_args  ):\n",
    "    \"\"\" \n",
    "    Conjugate Gradient\n",
    "    func:               the function to optimize It is called as \"value, gradient = func( x, 1 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "    \n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    m = len( initial_x )\n",
    "    iterations = 0\n",
    "    direction = np.asmatrix( np.zeros( initial_x.shape ) )\n",
    "    \n",
    "    # conjugate gradient updates\n",
    "    while True:\n",
    "        \n",
    "        value, gradient = func( x , 1 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.asarray( gradient )\n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        # if ( TODO: TERMINATION CRITERION ): break\n",
    "        \n",
    "        # beta = TODO: UPDATE BETA\n",
    "        \n",
    "        # reset after #(dimensions) iterations\n",
    "        if iterations % m == 0:\n",
    "            beta = 0\n",
    "        \n",
    "        # direction = TODO: FLETCHER-REEVES CONJUGATE GRADIENT UPDATE\n",
    "        \n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "        \n",
    "        x += t * direction\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return (x, values, runtimes, xs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, copy the function above but use the following Polak-Riberie formulae:\n",
    "$$ \\beta_{k+1}^{\\mathtt{PR}} = \\frac{\\nabla f_{k+1}^T(\\nabla f_{k+1} - \\nabla f_k)}{\\|f_k\\|^2}$$\n",
    "\n",
    "Observe that we applied the reset trick.\n",
    "It is worth reading more implementation hints in section [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(4 pts) Compare the efficiency (number of function/gradient evaluations) of FR and PR updates in CG method for Powell's optimization problem (PSF):\n",
    "$$ \\min_{-10 \\leq x_i \\leq 10} (x_1+10x_2)^2+5(x_3-x_4)^2+(x_2-2x_3)^4 + 10(x_1-x_4)^4,$$\n",
    "\n",
    "\n",
    "Observe that $f(X^*)=0$ for $X^*=0$.\n",
    "\n",
    "More info about PSF can be found, for example, here http://www.optimization-online.org/DB_FILE/2012/03/3382.pdf.\n",
    "\n",
    "Compare your results with [Nocedal, Wright, *Numerical Optimization*, Table 5.1] (row XPOWELL)\n",
    "<img width=50% src=\"https://i.ibb.co/6PVGJrS/Table51.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.3 (2 pts)**\n",
    "Show experimentally that affine invariance of Newton's method. \n",
    "\n",
    "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a convex function.\n",
    "Consider an affine transform $y\\mapsto Ay + b$, where $A \\in \\mathbb{R}^{n\\times n}$ is invertible and\n",
    "$b \\in \\mathbb R^n$.\n",
    "\n",
    "Define the function $g : \\mathbb R^n \\mapsto \\mathbb{R}$ by $g(y) = f(Ay + b)$.\n",
    "Denote by $x^{(k)}$ the k-th iterate of Newton‚Äôs method performed on $f$.\n",
    "Denote by $y^{(k)}$ the k-th iterate of Newton‚Äôs method performed on $g$.\n",
    "* Show that if $x^{(k)} = Ay^{(k)} + b$, then $x^{(k+1)} = Ay^{(k+1)} + b$.\n",
    "* Show that Newton's decrement does not depend on the coordinates, i.e., show that $Œª(x^{(k)}) = Œª(y^{(k)} ).$\n",
    "\n",
    "Together, this implies that Newton‚Äôs method is affine invariant. As an important consequence,\n",
    "Newton‚Äôs method cannot be improved by a change of coordinates, unlike gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.4 (2 pts)**\n",
    "Show experimentally that conjugate gradient method is *not* affine invariant.\n",
    "\n",
    "\n",
    "For example consider the quadratic (convex) function $f:\\mathbb R^n \\to \\mathbb R$ as follows\n",
    "$$ f(x) = \\frac12 x^T H x - c^T x,$$\n",
    "where $H$ positive semi-definite.\n",
    "\n",
    "Consider an affine transformation $y\\mapsto Ay$, where  $A \\in \\mathbb{R}^{n\\times n}$ is invertible:\n",
    "* Denote by $x^{(0)} , x^{(1)} , x^{(2)}$ the first three iterates of conjugate gradient descent on $f(x)$ initialized at $x^{(0)}$.\n",
    "* Now, let $y^{(0)}$ be the point such that $x^{(0)} = Ay^{(0)}$. Denote by $y^{(0)} , y^{(1)} , y^{(2)}$ the first three iterates of conjugate gradient descent on $g(y) = f(Ay)$ initialized at $y^{(0)}$.\n",
    "* Provide an explicit example of $H, A$ and $x^{(0)}$ such that $x^{(1)} \\neq Ay^{(1)}$ and $x^{(2)} \\neq Ay^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
